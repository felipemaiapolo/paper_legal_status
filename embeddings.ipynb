{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning embeddings (Word2vec/Doc2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import unidecode\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing: opening auxiliary files and defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that cleans texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(resulta):   \n",
    "    import copy \n",
    "   \n",
    "    result = copy.deepcopy(resulta)\n",
    "    \n",
    "    result=result.lower()\n",
    "    result=\" \"+result #colocando espaço no começo\n",
    "    result=re.sub('\\d', ' ', result)\n",
    "    result=result.replace(\"lei \", \"lei_\")\n",
    "    result=result.replace(\"lei nº \", \"lei_\")\n",
    "    result=result.replace(\"lei n.º\" ,\"lei_\")\n",
    "    result=result.replace(\"lei estadual nº \", \"lei_\") \n",
    "    result=result.replace(\"lei federal nº \", \"lei_\") \n",
    "    result=result.replace(\"lei municipal nº \", \"lei_\")\n",
    "    result=result.replace(\"fl. \", \"fls. \")\n",
    "    result=result.replace(\"fls. \", \"fls_\") \n",
    "    result=result.replace(\"p. \", \"pp. \")\n",
    "    result=result.replace(\"pp. \", \"pp_\")\n",
    "    result=result.replace(\"art. \", \"art_\") \n",
    "    result=result.replace(\"artigo \", \"art_\")\n",
    "    result=result.replace(\"inciso \", \"inciso_\") \n",
    "    result=result.replace(\"nº \", \"nº_\")\n",
    "    result=result.replace(\"n° \", \"nº_\")\n",
    "    result=result.replace(\"º \", \"º\")\n",
    "    result=result.replace(\"ª \", \"ª\")\n",
    "    result=result.replace(\"oab \", \"oab_\")\n",
    "    result=result.replace(\"r$ \", \"r$_\")\n",
    "    result=result.replace(\"\\n\", \" \")\n",
    "    result=result.replace(\"dr \", \"dr_\")\n",
    "    result=result.replace(\"dr. \", \"dr_\")\n",
    "    result=result.replace(\"dra \", \"dr_\")\n",
    "    result=result.replace(\"dra. \", \"dr_\")\n",
    "    result=result.replace(\"adv: \", \"adv_\") \n",
    "    \n",
    "    result=result.replace(\"/\", \" \")\n",
    "    result=result.replace(\"|\", \" \")\n",
    "    result=result.replace(\"+\", \" \")\n",
    "    result=result.replace(\".\", \" \")\n",
    "    result=result.replace(\",\", \" \")\n",
    "    result=result.replace(\":\", \" \")\n",
    "    result=result.replace(\";\", \" \")\n",
    "    result=result.replace(\"!\", \" \")\n",
    "    result=result.replace(\"?\", \" \")\n",
    "    result=result.replace(\">\", \" \")\n",
    "    result=result.replace(\"=\", \" \")\n",
    "    result=result.replace(\"§\", \" \")\n",
    "    result=result.replace(\" - \", \" \")\n",
    "    result=result.replace(\" _ \", \" \")\n",
    "    result=result.replace(\"&\", \" \")\n",
    "    result=result.replace(\"*\", \" \")\n",
    "    result=result.replace(\"(\", \" \")\n",
    "    result=result.replace(\")\", \" \")\n",
    "    result=result.replace(\"ª\", \" \")\n",
    "    result=result.replace(\"º\", \" \")\n",
    "    result=result.replace(\"%\", \" \")\n",
    "    result=result.replace(\"[\", \" \")\n",
    "    result=result.replace(\"]\", \" \")\n",
    "    result=result.replace(\"{\", \" \")\n",
    "    result=result.replace(\"}\", \" \")\n",
    "    result=result.replace(\"'\", \" \")\n",
    "    result=result.replace('\"', \" \")\n",
    "    result=result.replace(\"“\", \" \")\n",
    "    result=result.replace(\"”\", \" \")\n",
    "    result=re.sub(' +', ' ', result)\n",
    "    result=result+\" \" #colocando espaço no fim\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizador de textos\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def tokenize(txt):\n",
    "    texto=txt\n",
    "    texto=texto.split(' ') \n",
    "    tokens=[]\n",
    "    for t in texto:\n",
    "        if t not in stop_words: tokens.append(t)\n",
    "        else: pass\n",
    " \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening unlabeled motions dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bases/mov_treino.txt\", \"rb\") as fp:   # Unpickling\n",
    "    mov = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning texts and storing them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13 minutos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "texts_mov=[]\n",
    "\n",
    "for i in range(len(mov)):\n",
    "    for j in range(len(mov[i])):\n",
    "        texts_mov.append(clean(unidecode.unidecode(mov[i][j][1])))\n",
    "print(round((time.time() - start_time)/60,2),\"minutos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904255"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46 minutos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sentence_stream = [tokenize(doc) for doc in texts_mov[:]]\n",
    "print(round((time.time() - start_time)/60,2),\"minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning which combinations of words should be considered as unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.15 minutos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "bigrams=Phrases(sentence_stream)#, threshold=1)\n",
    "bibigrams=Phrases(bigrams[sentence_stream])# , threshold=1)\n",
    "print(round((time.time() - start_time)/60,2),\"minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, transforming the texts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1 minutos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(len(sentence_stream)):\n",
    "    sentence_stream[i]=bibigrams[bigrams[sentence_stream[i]]]\n",
    "print(round((time.time() - start_time)/60,2),\"minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.save('modelos/bigrams_mov')\n",
    "bibigrams.save('modelos/bibigrams_mov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Word2vec/Doc2vec representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[100] \n",
    "windows=[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentence_stream)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.97 minutos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for s in sizes:\n",
    "    for w in windows:\n",
    "        model = Doc2Vec(documents, vector_size=s, window=w, seed=1)\n",
    "        model.save('modelos/doc2vec_mov_'+str(s)+'_'+str(w)+'_V5')\n",
    "        \n",
    "print(round((time.time() - start_time)/60,2),\"minutos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
