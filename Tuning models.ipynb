{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning models for proceedings classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best hyperparameters for CNN, Doc2Vec & TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(9)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maiapolo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import unidecode\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy \n",
    "from random import choice\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import keras\n",
    "from itertools import product\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bases/mov.txt\", \"rb\") as fp:   # Legal proceedings\n",
    "    mov = pickle.load(fp)\n",
    "\n",
    "with open(\"bases/tags.txt\", \"rb\") as fp:   # Tags\n",
    "    tags = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening models that identify combinations of words as unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams=Phrases.load('modelos/bigrams_mov')\n",
    "bibigrams=Phrases.load('modelos/bibigrams_mov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting motions in cronological order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['14-11-2018',\n",
       "  'Tipo do Movimento:Ato Ordinatório Praticado Descrição:Certifico e dou fé que as custas pendentes de fls 685 não foram recolhidas pelas parte GILMAR RODRIGUES REGO e BRUNO MEDEIROS.'],\n",
       " ['23-07-2018',\n",
       "  'Tipo do Movimento:Juntada - Petição Descrição da juntada:Documento eletrônico juntado de forma automática.'],\n",
       " ['16-07-2018',\n",
       "  'Tipo do Movimento:Publicado\\xa0 Atos da Serventia Folhas do DJERJ.:388/416'],\n",
       " ['29-06-2018', 'Tipo do Movimento:Enviado para publicação ']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mov[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mov:\n",
    "    m.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10-09-2010',\n",
       "  'Tipo do Movimento:Distribuição Sorteio Serventia:Cartório da 34ª Vara Cível - 34ª Vara Cível'],\n",
       " ['21-09-2010',\n",
       "  'Tipo do Movimento:Conclusão ao Juiz Juiz:JOAO MARCOS DE CASTELLO BRANCO FANTINATO'],\n",
       " ['23-09-2010',\n",
       "  'Tipo do Movimento:Despacho - Proferido despacho de mero expediente Descrição:Defiro JG. \\r\\n\\r\\nIndefiro a antecipação dos efeitos da tutela, eis que as alegações do autor carecem de dilação probatória.\\r\\n\\r\\nCite-se.'],\n",
       " ['23-09-2010', 'Tipo do Movimento:Enviado para publicação ']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mov[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that cleans texts:\n",
    "def clean(resulta):   \n",
    "    import copy \n",
    "   \n",
    "    result = copy.deepcopy(resulta)\n",
    "    \n",
    "    result=result.lower()\n",
    "    result=re.sub('\\d', ' ', result)\n",
    "    result=result.replace(\"lei \", \"lei_\")\n",
    "    result=result.replace(\"lei nº \", \"lei_\")\n",
    "    result=result.replace(\"lei n.º\" ,\"lei_\")\n",
    "    result=result.replace(\"lei estadual nº \", \"lei_\") \n",
    "    result=result.replace(\"lei federal nº \", \"lei_\") \n",
    "    result=result.replace(\"lei municipal nº \", \"lei_\")\n",
    "    result=result.replace(\"fl. \", \"fls. \")\n",
    "    result=result.replace(\"fls. \", \"fls_\") \n",
    "    result=result.replace(\"p. \", \"pp. \")\n",
    "    result=result.replace(\"pp. \", \"pp_\")\n",
    "    result=result.replace(\"art. \", \"art_\") \n",
    "    result=result.replace(\"artigo \", \"art_\")\n",
    "    result=result.replace(\"inciso \", \"inciso_\") \n",
    "    result=result.replace(\"nº \", \"nº_\")\n",
    "    result=result.replace(\"n° \", \"nº_\")\n",
    "    result=result.replace(\"º \", \"º\")\n",
    "    result=result.replace(\"ª \", \"ª\")\n",
    "    result=result.replace(\"oab \", \"oab_\")\n",
    "    result=result.replace(\"r$ \", \"r$_\")\n",
    "    result=result.replace(\"\\n\", \" \")\n",
    "    result=result.replace(\"dr \", \"dr_\")\n",
    "    result=result.replace(\"dr. \", \"dr_\")\n",
    "    result=result.replace(\"dra \", \"dr_\")\n",
    "    result=result.replace(\"dra. \", \"dr_\")\n",
    "    result=result.replace(\"adv: \", \"adv_\") \n",
    "    \n",
    "    result=result.replace(\"/\", \" \")\n",
    "    result=result.replace(\"|\", \" \")\n",
    "    result=result.replace(\"+\", \" \")\n",
    "    result=result.replace(\".\", \" \")\n",
    "    result=result.replace(\",\", \" \")\n",
    "    result=result.replace(\":\", \" \")\n",
    "    result=result.replace(\";\", \" \")\n",
    "    result=result.replace(\"!\", \" \")\n",
    "    result=result.replace(\"?\", \" \")\n",
    "    result=result.replace(\">\", \" \")\n",
    "    result=result.replace(\"=\", \" \")\n",
    "    result=result.replace(\"§\", \" \")\n",
    "    result=result.replace(\" - \", \" \")\n",
    "    result=result.replace(\" _ \", \" \")\n",
    "    result=result.replace(\"&\", \" \")\n",
    "    result=result.replace(\"*\", \" \")\n",
    "    result=result.replace(\"(\", \" \")\n",
    "    result=result.replace(\")\", \" \")\n",
    "    result=result.replace(\"ª\", \" \")\n",
    "    result=result.replace(\"º\", \" \")\n",
    "    result=result.replace(\"%\", \" \")\n",
    "    result=result.replace(\"[\", \" \")\n",
    "    result=result.replace(\"]\", \" \")\n",
    "    result=result.replace(\"{\", \" \")\n",
    "    result=result.replace(\"}\", \" \")\n",
    "    result=result.replace(\"'\", \" \")\n",
    "    result=result.replace('\"', \" \")\n",
    "    result=result.replace(\"“\", \" \")\n",
    "    result=result.replace(\"”\", \" \")\n",
    "    result=re.sub(' +', ' ', result)\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def tokenize(txt):\n",
    "    texto=unidecode.unidecode(txt) #decodificando\n",
    "    texto=clean(texto) #limpando texto\n",
    "    texto=texto.split(' ') \n",
    "    \n",
    "    tokens=[]\n",
    "    for t in texto:\n",
    "        if t not in stop_words: tokens.append(t)\n",
    "        else: pass\n",
    "        \n",
    "    tokens=bibigrams[bigrams[tokens]]\n",
    " \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening Word2Vec pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = KeyedVectors.load(\"modelos/doc2vec_mov_100_5_V5\")\n",
    "\n",
    "embed_dim=np.shape(word['juiz'])[0]\n",
    "embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for embedding (normalized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb(s):\n",
    "    return(word[s]/np.sqrt(word[s]@word[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform a sequence of tokens to the matrix form, \n",
    "#where each line is given by an embedded token\n",
    "def tokens2matrix(tokens):\n",
    "    matrix=[]\n",
    "    for s in tokens: \n",
    "        if (s in word_vectors.vocab) and (s not in stop_words): #in vocab and no stopwords\n",
    "            matrix.append(list(emb(s)))\n",
    "        else: pass\n",
    "    return(np.array(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that checks if in a given text there is at least one word in the vocabulary\n",
    "def in_vocab(texto):\n",
    "    for s in tokenize(texto):\n",
    "        if (s in word_vectors.vocab) and (s not in stop_words): return(True)\n",
    "        else: pass\n",
    "    return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_cnn(mov,tags,len1=5,len2=30):\n",
    "    \n",
    "    #len1: let's take the last len1 motions in each proceeding\n",
    "    #len2: let's take the first len2 tokens in each motion\n",
    "    \n",
    "    #indexes of valid legal proceedings (with motions and tags)\n",
    "    index=[]\n",
    "    for i in range(len(mov)):\n",
    "        if 0<len(mov[i]) and tags[i]!=\"\": index.append(i) \n",
    "        else: pass\n",
    "\n",
    "    print(\"Number of valid proceedings (with motions and tags):\",len(index),\"\\n\")\n",
    "\n",
    "    #organizing motions in X and tags in y\n",
    "    X=np.zeros((len(index),len1,len2,embed_dim))\n",
    "    y=[]\n",
    "    cont=0\n",
    "    \n",
    "    #get numerical X embedding the tokens from those motions (from valid proceedings):\n",
    "    for i in index:\n",
    "        temp=[]\n",
    "        y.append(tags[i])\n",
    "\n",
    "        mov[i]=mov[i][-len1::] #let's take the last len1 motions in each proceeding\n",
    "\n",
    "        for j in range(len(mov[i])):\n",
    "\n",
    "            if in_vocab(mov[i][j][1]):  #if there is at least one word in the vocabulary\n",
    "                temp=tokens2matrix(tokenize(mov[i][j][1])[:len2]) #let's take the first len2 tokens in each motion and turn them into a matrix l2x100\n",
    "                X[cont][j][:np.shape(temp)[0]]=temp\n",
    "            else: pass\n",
    "\n",
    "\n",
    "        #counter\n",
    "        cont+=1\n",
    "        if cont%int(len(index)/5)==0: print(round(100*cont/len(index),0),\"% concluded\")\n",
    "        else: pass\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid proceedings (with motions and tags): 6449 \n",
      "\n",
      "20.0 % concluded\n",
      "40.0 % concluded\n",
      "60.0 % concluded\n",
      "80.0 % concluded\n",
      "100.0 % concluded\n"
     ]
    }
   ],
   "source": [
    "len1=5 #vamos pegar somente as últimas l1 movimentações\n",
    "len2=30 #vamos pegar somente os primeiros l2 tokens de cada movimentação\n",
    "\n",
    "X,y=get_X_y_cnn(mov,tags,len1,len2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning y into numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "#\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset in train, test and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 30, 100), (4514,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y)\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=22)\n",
    "#\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 6)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_grid(dictionary):\n",
    "       return pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                           columns=dictionary.keys())\n",
    "\n",
    "hyper = {'ks': [3,5,8,12], #kernels\n",
    "         'neurons': [10,30,50,75,100], #hidden LSTM\n",
    "         'lambdas': [.0,.0001,.0003,.0005,.0007,.0009,.0011,.0013, .0015, .0016, .0018, .002, .0025, .003], #regularization\n",
    "         'score': [0], \n",
    "         'lower_ci': [0], \n",
    "         'upper_ci': [0]}\n",
    "\n",
    "hyper=expand_grid(hyper)\n",
    "hyper=hyper[['ks','neurons','lambdas','score','lower_ci','upper_ci']]\n",
    "\n",
    "np.shape(hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "Adam=keras.optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % concluded in 1.91 minutes\n",
      "10.0 % concluded in 60.94 minutes\n",
      "20.0 % concluded in 122.12 minutes\n",
      "30.0 % concluded in 198.04 minutes\n",
      "40.0 % concluded in 279.9 minutes\n",
      "50.0 % concluded in 369.87 minutes\n",
      "60.0 % concluded in 466.01 minutes\n",
      "70.0 % concluded in 556.53 minutes\n",
      "80.0 % concluded in 651.08 minutes\n",
      "90.0 % concluded in 751.99 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "for i in range(np.shape(hyper)[0]):\n",
    "    \n",
    "    k=hyper.loc[i,'ks'] \n",
    "    neuron=hyper.loc[i,'neurons'] \n",
    "    lamb=hyper.loc[i,'lambdas'] \n",
    "    \n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "\n",
    "    ### model for features extraction\n",
    "    inputs = Input(shape=np.shape(X_train)[1:])\n",
    "    conv = TimeDistributed(Conv1D(k, 1, activation='relu',kernel_constraint=unit_norm(axis=1), use_bias=False))(inputs)\n",
    "    pool = TimeDistributed(GlobalMaxPooling1D())(conv)\n",
    "    #\n",
    "    model_feat = Model(inputs, pool)\n",
    "\n",
    "    ### model for classification\n",
    "    pooled_inputs = Input(shape=(5, k))\n",
    "    lstm = LSTM(neuron, kernel_regularizer=l1(lamb))(pooled_inputs)\n",
    "    soft = Dense(num_classes, activation='softmax')(lstm)\n",
    "    #\n",
    "    model_classific = Model(pooled_inputs, soft)\n",
    "\n",
    "    ### final model\n",
    "    outputs = model_classific(model_feat(inputs))\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    #compiling\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #Running\n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "\n",
    "    modelo=model.fit(X_train, y_train2, epochs=50,\n",
    "                                              batch_size=500,\n",
    "                                              shuffle=True,\n",
    "                                              verbose=False,\n",
    "                                              validation_data=(X_val, y_val2))\n",
    "    \n",
    "    p=modelo.history['val_accuracy'][-1]\n",
    "    hyper.loc[i,'score']=p\n",
    "    hyper.loc[i,'lower_ci']=p-1.96*np.sqrt((p*(1-p)/np.shape(y_val)[0]))\n",
    "    hyper.loc[i,'upper_ci']=p+1.96*np.sqrt((p*(1-p)/np.shape(y_val)[0]))\n",
    "    \n",
    "    hyper.to_csv('hyper_cnn')\n",
    "    \n",
    "    #progress\n",
    "    if i%int(np.shape(hyper)[0]/10)==0: print(round(100*i/np.shape(hyper)[0],0),\"% concluded in\", np.round((time.time() - start_time)/60,2),\"minutes\")\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ks</th>\n",
       "      <th>neurons</th>\n",
       "      <th>lambdas</th>\n",
       "      <th>score</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>upper_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.894574</td>\n",
       "      <td>0.870873</td>\n",
       "      <td>0.918274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.894574</td>\n",
       "      <td>0.870873</td>\n",
       "      <td>0.918274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>113</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.894574</td>\n",
       "      <td>0.870873</td>\n",
       "      <td>0.918274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.894574</td>\n",
       "      <td>0.870873</td>\n",
       "      <td>0.918274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.894574</td>\n",
       "      <td>0.870873</td>\n",
       "      <td>0.918274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.896124</td>\n",
       "      <td>0.872578</td>\n",
       "      <td>0.919670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.896124</td>\n",
       "      <td>0.872578</td>\n",
       "      <td>0.919670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.896124</td>\n",
       "      <td>0.872578</td>\n",
       "      <td>0.919670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.896124</td>\n",
       "      <td>0.872578</td>\n",
       "      <td>0.919670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.874285</td>\n",
       "      <td>0.921064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>216</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.874285</td>\n",
       "      <td>0.921064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.874285</td>\n",
       "      <td>0.921064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.874285</td>\n",
       "      <td>0.921064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.874285</td>\n",
       "      <td>0.921064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.875993</td>\n",
       "      <td>0.922457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>215</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.900775</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.923848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.879414</td>\n",
       "      <td>0.925237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.903876</td>\n",
       "      <td>0.881128</td>\n",
       "      <td>0.926624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.903876</td>\n",
       "      <td>0.881128</td>\n",
       "      <td>0.926624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.905426</td>\n",
       "      <td>0.882843</td>\n",
       "      <td>0.928010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.905426</td>\n",
       "      <td>0.882843</td>\n",
       "      <td>0.928010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>0.929393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>0.929393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.908527</td>\n",
       "      <td>0.886279</td>\n",
       "      <td>0.930775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.910078</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.932155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.910078</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.932155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.911628</td>\n",
       "      <td>0.889723</td>\n",
       "      <td>0.933533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.911628</td>\n",
       "      <td>0.889723</td>\n",
       "      <td>0.933533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.893175</td>\n",
       "      <td>0.936282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.916279</td>\n",
       "      <td>0.894904</td>\n",
       "      <td>0.937654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.916279</td>\n",
       "      <td>0.894904</td>\n",
       "      <td>0.937654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.919380</td>\n",
       "      <td>0.898369</td>\n",
       "      <td>0.940391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.920930</td>\n",
       "      <td>0.900105</td>\n",
       "      <td>0.941756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.920930</td>\n",
       "      <td>0.900105</td>\n",
       "      <td>0.941756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.922481</td>\n",
       "      <td>0.901843</td>\n",
       "      <td>0.943118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.922481</td>\n",
       "      <td>0.901843</td>\n",
       "      <td>0.943118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.925581</td>\n",
       "      <td>0.905327</td>\n",
       "      <td>0.945836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.927132</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.947191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ks  neurons  lambdas     score  lower_ci  upper_ci\n",
       "72           72   8       50   0.0005  0.894574  0.870873  0.918274\n",
       "71           71   8       30   0.0005  0.894574  0.870873  0.918274\n",
       "113         113   8       75   0.0009  0.894574  0.870873  0.918274\n",
       "88           88   5       75   0.0007  0.894574  0.870873  0.918274\n",
       "14           14   8      100   0.0000  0.894574  0.870873  0.918274\n",
       "196         196  12       30   0.0016  0.896124  0.872578  0.919670\n",
       "236         236  12       30   0.0020  0.896124  0.872578  0.919670\n",
       "48           48   5       75   0.0003  0.896124  0.872578  0.919670\n",
       "93           93   8       75   0.0007  0.896124  0.872578  0.919670\n",
       "156         156  12       30   0.0013  0.897674  0.874285  0.921064\n",
       "216         216  12       30   0.0018  0.897674  0.874285  0.921064\n",
       "116         116  12       30   0.0009  0.897674  0.874285  0.921064\n",
       "136         136  12       30   0.0011  0.897674  0.874285  0.921064\n",
       "28           28   5       75   0.0001  0.897674  0.874285  0.921064\n",
       "96           96  12       30   0.0007  0.899225  0.875993  0.922457\n",
       "76           76  12       30   0.0005  0.900775  0.877703  0.923848\n",
       "73           73   8       75   0.0005  0.900775  0.877703  0.923848\n",
       "215         215  12       10   0.0018  0.900775  0.877703  0.923848\n",
       "77           77  12       50   0.0005  0.900775  0.877703  0.923848\n",
       "56           56  12       30   0.0003  0.900775  0.877703  0.923848\n",
       "19           19  12      100   0.0000  0.900775  0.877703  0.923848\n",
       "176         176  12       30   0.0015  0.900775  0.877703  0.923848\n",
       "31           31   8       30   0.0001  0.902326  0.879414  0.925237\n",
       "8             8   5       75   0.0000  0.902326  0.879414  0.925237\n",
       "53           53   8       75   0.0003  0.902326  0.879414  0.925237\n",
       "11           11   8       30   0.0000  0.902326  0.879414  0.925237\n",
       "51           51   8       30   0.0003  0.902326  0.879414  0.925237\n",
       "36           36  12       30   0.0001  0.902326  0.879414  0.925237\n",
       "16           16  12       30   0.0000  0.902326  0.879414  0.925237\n",
       "29           29   5      100   0.0001  0.903876  0.881128  0.926624\n",
       "98           98  12       75   0.0007  0.903876  0.881128  0.926624\n",
       "38           38  12       75   0.0001  0.905426  0.882843  0.928010\n",
       "58           58  12       75   0.0003  0.905426  0.882843  0.928010\n",
       "57           57  12       50   0.0003  0.906977  0.884560  0.929393\n",
       "17           17  12       50   0.0000  0.906977  0.884560  0.929393\n",
       "78           78  12       75   0.0005  0.908527  0.886279  0.930775\n",
       "18           18  12       75   0.0000  0.910078  0.888000  0.932155\n",
       "195         195  12       10   0.0016  0.910078  0.888000  0.932155\n",
       "37           37  12       50   0.0001  0.911628  0.889723  0.933533\n",
       "175         175  12       10   0.0015  0.911628  0.889723  0.933533\n",
       "155         155  12       10   0.0013  0.914729  0.893175  0.936282\n",
       "13           13   8       75   0.0000  0.916279  0.894904  0.937654\n",
       "33           33   8       75   0.0001  0.916279  0.894904  0.937654\n",
       "135         135  12       10   0.0011  0.919380  0.898369  0.940391\n",
       "115         115  12       10   0.0009  0.920930  0.900105  0.941756\n",
       "75           75  12       10   0.0005  0.920930  0.900105  0.941756\n",
       "95           95  12       10   0.0007  0.922481  0.901843  0.943118\n",
       "55           55  12       10   0.0003  0.922481  0.901843  0.943118\n",
       "15           15  12       10   0.0000  0.925581  0.905327  0.945836\n",
       "35           35  12       10   0.0001  0.927132  0.907072  0.947191"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc2Vec.load(\"modelos/doc2vec_mov_100_5_V5\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_doc(tokens, model, normalize=True): #a opção normalize pode melhorar a qualidade do embedding\n",
    "    \n",
    "    model.random.seed(42)\n",
    "    x=model.infer_vector(tokens, steps=20)\n",
    "    \n",
    "    if normalize: return(x/(np.sqrt(x@x)))\n",
    "    else: return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_doc(mov,tags,len1=5):\n",
    "    \n",
    "    #len1: let's take the last l1 motions in each proceeding\n",
    "    \n",
    "    #indexes of valid legal proceedings (with motions and tags)\n",
    "    index=[]\n",
    "    for i in range(len(mov)):\n",
    "        if 0<len(mov[i]) and tags[i]!=\"\": index.append(i) \n",
    "        else: pass\n",
    "\n",
    "    print(\"Number of valid proceedings (with motions and tags):\",len(index),\"\\n\")\n",
    "\n",
    "    #organizing motions in X and tags in y\n",
    "    X=np.zeros((len(index),len1,embed_dim))\n",
    "    y=[]\n",
    "    cont=0\n",
    "    \n",
    "    #get numerical X embedding the tokens from those motions (from valid proceedings):\n",
    "    for i in index:\n",
    "        temp=[]\n",
    "        y.append(tags[i])\n",
    "\n",
    "        mov[i]=mov[i][-len1::] #let's take the last l1 motions in each proceeding\n",
    "\n",
    "        for j in range(len(mov[i])):\n",
    "            temp=emb_doc(tokenize(mov[i][j][1]), doc)\n",
    "            X[cont][j][:np.shape(temp)[0]]=temp\n",
    "\n",
    "        #counter\n",
    "        cont+=1\n",
    "        if cont%int(len(index)/5)==0: print(round(100*cont/len(index),0),\"% concluded\")\n",
    "        else: pass\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid proceedings (with motions and tags): 6449 \n",
      "\n",
      "20.0 % concluded\n",
      "40.0 % concluded\n",
      "60.0 % concluded\n",
      "80.0 % concluded\n",
      "100.0 % concluded\n"
     ]
    }
   ],
   "source": [
    "len1=5 #vamos pegar somente as últimas len1 movimentações\n",
    "\n",
    "X,y=get_X_y_doc(mov,tags,len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "#\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 100), (4514,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y)\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=22)\n",
    "#\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid(dictionary):\n",
    "       return pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                           columns=dictionary.keys())\n",
    "\n",
    "hyper = {'neurons': [10,30,50,75,100], #hidden LSTM\n",
    "         'lambdas': [.0,.0001,.0003,.0005,.0007,.0009,.0011,.0013, .0015, .0016, .0018, .002, .0025, .003], #regularization\n",
    "         'score': [0], \n",
    "         'lower_ci': [0], \n",
    "         'upper_ci': [0]}\n",
    "\n",
    "hyper=expand_grid(hyper)\n",
    "hyper=hyper[['neurons','lambdas','score','lower_ci','upper_ci']]\n",
    "\n",
    "np.shape(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam=keras.optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % concluded in 0.25 minutes\n",
      "10.0 % concluded in 2.13 minutes\n",
      "20.0 % concluded in 4.19 minutes\n",
      "30.0 % concluded in 6.92 minutes\n",
      "40.0 % concluded in 9.9 minutes\n",
      "50.0 % concluded in 13.63 minutes\n",
      "60.0 % concluded in 17.62 minutes\n",
      "70.0 % concluded in 22.42 minutes\n",
      "80.0 % concluded in 27.61 minutes\n",
      "90.0 % concluded in 33.61 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "for i in range(np.shape(hyper)[0]):\n",
    "    \n",
    "    neuron=hyper.loc[i,'neurons'] \n",
    "    lamb=hyper.loc[i,'lambdas'] \n",
    "    \n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "\n",
    "    ### model for features extraction\n",
    "    inputs = Input(shape=np.shape(X_train)[1:]) \n",
    "    time_layer = TimeDistributed(Lambda(lambda x: x))(inputs) #Dense(neuron,activation='relu', activity_regularizer=l1(lamb1))\n",
    "    lstm = LSTM(neuron, kernel_regularizer=l1(lamb))(time_layer)\n",
    "    soft = Dense(num_classes, activation='softmax')(lstm)\n",
    "\n",
    "    ### final model\n",
    "    model = Model(inputs, soft)\n",
    "\n",
    "    #compiling\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
    "\n",
    "    #Running\n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "\n",
    "    modelo=model.fit(X_train, y_train2, epochs=50,\n",
    "                                              batch_size=500,\n",
    "                                              shuffle=True,\n",
    "                                              verbose=False,\n",
    "                                              validation_data=(X_val, y_val2))\n",
    "    \n",
    "    p=modelo.history['val_accuracy'][-1]\n",
    "    hyper.loc[i,'score']=p\n",
    "    hyper.loc[i,'lower_ci']=p-1.96*np.sqrt((p*(1-p)/np.shape(y_val)[0]))\n",
    "    hyper.loc[i,'upper_ci']=p+1.96*np.sqrt((p*(1-p)/np.shape(y_val)[0]))\n",
    "   \n",
    "    #progress\n",
    "    if i%int(np.shape(hyper)[0]/10)==0: print(round(100*i/np.shape(hyper)[0],0),\"% concluded in\", np.round((time.time() - start_time)/60,2),\"minutes\")\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neurons</th>\n",
       "      <th>lambdas</th>\n",
       "      <th>score</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>upper_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.820155</td>\n",
       "      <td>0.790515</td>\n",
       "      <td>0.849795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.820155</td>\n",
       "      <td>0.790515</td>\n",
       "      <td>0.849795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.820155</td>\n",
       "      <td>0.790515</td>\n",
       "      <td>0.849795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.792166</td>\n",
       "      <td>0.851245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.823256</td>\n",
       "      <td>0.793817</td>\n",
       "      <td>0.852694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.823256</td>\n",
       "      <td>0.793817</td>\n",
       "      <td>0.852694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.824806</td>\n",
       "      <td>0.795469</td>\n",
       "      <td>0.854143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.824806</td>\n",
       "      <td>0.795469</td>\n",
       "      <td>0.854143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.826357</td>\n",
       "      <td>0.797123</td>\n",
       "      <td>0.855591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.826357</td>\n",
       "      <td>0.797123</td>\n",
       "      <td>0.855591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.826357</td>\n",
       "      <td>0.797123</td>\n",
       "      <td>0.855591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.826357</td>\n",
       "      <td>0.797123</td>\n",
       "      <td>0.855591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.827907</td>\n",
       "      <td>0.798776</td>\n",
       "      <td>0.857038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.827907</td>\n",
       "      <td>0.798776</td>\n",
       "      <td>0.857038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.827907</td>\n",
       "      <td>0.798776</td>\n",
       "      <td>0.857038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.829457</td>\n",
       "      <td>0.800431</td>\n",
       "      <td>0.858484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.832558</td>\n",
       "      <td>0.803743</td>\n",
       "      <td>0.861373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.832558</td>\n",
       "      <td>0.803743</td>\n",
       "      <td>0.861373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.834109</td>\n",
       "      <td>0.805401</td>\n",
       "      <td>0.862816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.835659</td>\n",
       "      <td>0.807059</td>\n",
       "      <td>0.864259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.808718</td>\n",
       "      <td>0.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.841860</td>\n",
       "      <td>0.813702</td>\n",
       "      <td>0.870019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.843411</td>\n",
       "      <td>0.815364</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.843411</td>\n",
       "      <td>0.815364</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.843411</td>\n",
       "      <td>0.815364</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.817028</td>\n",
       "      <td>0.872894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>0.817028</td>\n",
       "      <td>0.872894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.846512</td>\n",
       "      <td>0.818693</td>\n",
       "      <td>0.874330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.851163</td>\n",
       "      <td>0.823694</td>\n",
       "      <td>0.878632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.842112</td>\n",
       "      <td>0.894322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neurons  lambdas     score  lower_ci  upper_ci\n",
       "62      100   0.0011  0.820155  0.790515  0.849795\n",
       "21       30   0.0013  0.820155  0.790515  0.849795\n",
       "9        10   0.0016  0.820155  0.790515  0.849795\n",
       "20       30   0.0011  0.821705  0.792166  0.851245\n",
       "31       50   0.0005  0.823256  0.793817  0.852694\n",
       "33       50   0.0009  0.823256  0.793817  0.852694\n",
       "11       10   0.0020  0.824806  0.795469  0.854143\n",
       "19       30   0.0009  0.824806  0.795469  0.854143\n",
       "6        10   0.0011  0.826357  0.797123  0.855591\n",
       "56      100   0.0000  0.826357  0.797123  0.855591\n",
       "32       50   0.0007  0.826357  0.797123  0.855591\n",
       "43       75   0.0001  0.826357  0.797123  0.855591\n",
       "10       10   0.0018  0.827907  0.798776  0.857038\n",
       "44       75   0.0003  0.827907  0.798776  0.857038\n",
       "7        10   0.0013  0.827907  0.798776  0.857038\n",
       "57      100   0.0001  0.829457  0.800431  0.858484\n",
       "5        10   0.0009  0.832558  0.803743  0.861373\n",
       "17       30   0.0005  0.832558  0.803743  0.861373\n",
       "18       30   0.0007  0.834109  0.805401  0.862816\n",
       "42       75   0.0000  0.835659  0.807059  0.864259\n",
       "15       30   0.0001  0.837209  0.808718  0.865700\n",
       "28       50   0.0000  0.841860  0.813702  0.870019\n",
       "29       50   0.0001  0.843411  0.815364  0.871457\n",
       "3        10   0.0005  0.843411  0.815364  0.871457\n",
       "16       30   0.0003  0.843411  0.815364  0.871457\n",
       "4        10   0.0007  0.844961  0.817028  0.872894\n",
       "2        10   0.0003  0.844961  0.817028  0.872894\n",
       "0        10   0.0000  0.846512  0.818693  0.874330\n",
       "1        10   0.0001  0.851163  0.823694  0.878632\n",
       "14       30   0.0000  0.868217  0.842112  0.894322"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizador de textos\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def tokenize_tfidf(txt):\n",
    "    texto=txt\n",
    "    texto=texto.split(' ') \n",
    "    texto=bibigrams[bigrams[texto]]\n",
    "    tokens=[]\n",
    "    for t in texto:\n",
    "        if t not in stop_words: tokens.append(t)\n",
    "        else: pass\n",
    " \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('modelos/tfidf_mov_V1.sav', 'rb'))\n",
    "tfidf_dim=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_tfidf(mov,tags,len1=5):\n",
    "    \n",
    "    #len1: let's take the last l1 motions in each proceeding\n",
    "\n",
    "    #indexes of valid legal proceedings (with motions and tags)\n",
    "    index=[]\n",
    "    for i in range(len(mov)):\n",
    "        if 0<len(mov[i]) and tags[i]!=\"\": index.append(i) \n",
    "        else: pass\n",
    "\n",
    "    print(\"Number of valid proceedings (with motions and tags):\",len(index),\"\\n\")\n",
    "\n",
    "    #organizing motions in X and tags in y\n",
    "    X=np.zeros((len(index),len1,tfidf_dim))\n",
    "    y=[]\n",
    "    cont=0\n",
    "    \n",
    "    #get numerical X embedding the tokens from those motions (from valid proceedings):\n",
    "    for i in index:\n",
    "        temp=[]\n",
    "        y.append(tags[i])\n",
    "\n",
    "        mov[i]=mov[i][-len1::] #let's take the last l1 motions in each proceeding\n",
    "\n",
    "        for j in range(len(mov[i])):\n",
    "            temp=tfidf.transform([mov[i][j][1]]).toarray()[0]\n",
    "            X[cont][j][:np.shape(temp)[0]]=temp\n",
    "\n",
    "        #counter\n",
    "        cont+=1\n",
    "        if cont%int(len(index)/5)==0: print(round(100*cont/len(index),0),\"% concluded\")\n",
    "        else: pass\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid proceedings (with motions and tags): 6449 \n",
      "\n",
      "20.0 % concluded\n",
      "40.0 % concluded\n",
      "60.0 % concluded\n",
      "80.0 % concluded\n",
      "100.0 % concluded\n"
     ]
    }
   ],
   "source": [
    "len1=5 #vamos pegar somente as últimas len1 movimentações\n",
    "\n",
    "X,y=get_X_y_tfidf(mov,tags,len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "#\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 2000), (4514,))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y)\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=22)\n",
    "#\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_grid(dictionary):\n",
    "       return pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                           columns=dictionary.keys())\n",
    "\n",
    "hyper = {'neurons': [10,30,50,75,100], #hidden LSTM\n",
    "         'lambdas': [.0,.0001,.0003,.0005,.0007,.0009,.0011,.0013, .0015, .0016, .0018, .002, .0025, .003], #regularization\n",
    "         'score': [0], \n",
    "         'lower_ci': [0], \n",
    "         'upper_ci': [0]}\n",
    "\n",
    "hyper=expand_grid(hyper)\n",
    "hyper=hyper[['neurons','lambdas','score','lower_ci','upper_ci']]\n",
    "\n",
    "np.shape(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "Adam=keras.optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "for i in range(68,70):\n",
    "    \n",
    "    neuron=hyper.loc[i,'neurons'] \n",
    "    lamb=hyper.loc[i,'lambdas'] \n",
    "    \n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "\n",
    "    ### model for features extraction\n",
    "    inputs = Input(shape=np.shape(X_train)[1:]) \n",
    "    time_layer = TimeDistributed(Lambda(lambda x: x))(inputs) #Dense(neuron,activation='relu', activity_regularizer=l1(lamb1))\n",
    "    lstm = LSTM(neuron, kernel_regularizer=l1(lamb))(time_layer)\n",
    "    soft = Dense(num_classes, activation='softmax')(lstm)\n",
    "\n",
    "    ### final model\n",
    "    model = Model(inputs, soft)\n",
    "\n",
    "    #compiling\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
    "\n",
    "    #Running\n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "\n",
    "    modelo=model.fit(X_train, y_train2, epochs=50,\n",
    "                                              batch_size=500,\n",
    "                                              shuffle=True,\n",
    "                                              verbose=False,\n",
    "                                              validation_data=(X_val, y_val2))\n",
    "    \n",
    "    p=modelo.history['val_accuracy'][-1]\n",
    "    hyper.loc[i,'score']=p\n",
    "    hyper.loc[i,'lower_ci']=p-1.96*np.sqrt((p*(1-p)/np.shape(y_val)[0]))\n",
    "    hyper.loc[i,'upper_ci']=p+1.96*np.sqrt((p*(1-p)/np.shape(y_val)[0]))\n",
    "    \n",
    "\n",
    "    hyper.to_csv('hyper_tfidf')\n",
    "    #progress\n",
    "    if i%int(np.shape(hyper)[0]/10)==0: print(round(100*i/np.shape(hyper)[0],0),\"% concluded in\", np.round((time.time() - start_time)/60,2),\"minutes\")\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>neurons</th>\n",
       "      <th>lambdas</th>\n",
       "      <th>score</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>upper_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.927132</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.947191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.927132</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.947191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.928682</td>\n",
       "      <td>0.908821</td>\n",
       "      <td>0.948544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.928682</td>\n",
       "      <td>0.908821</td>\n",
       "      <td>0.948544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.928682</td>\n",
       "      <td>0.908821</td>\n",
       "      <td>0.948544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.928682</td>\n",
       "      <td>0.908821</td>\n",
       "      <td>0.948544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.928682</td>\n",
       "      <td>0.908821</td>\n",
       "      <td>0.948544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.949893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.931783</td>\n",
       "      <td>0.912326</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.914083</td>\n",
       "      <td>0.952584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.914083</td>\n",
       "      <td>0.952584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.914083</td>\n",
       "      <td>0.952584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.915842</td>\n",
       "      <td>0.953925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.915842</td>\n",
       "      <td>0.953925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  neurons  lambdas     score  lower_ci  upper_ci\n",
       "19          19       30   0.0009  0.927132  0.907072  0.947191\n",
       "33          33       50   0.0009  0.927132  0.907072  0.947191\n",
       "35          35       50   0.0013  0.928682  0.908821  0.948544\n",
       "7            7       10   0.0013  0.928682  0.908821  0.948544\n",
       "39          39       50   0.0020  0.928682  0.908821  0.948544\n",
       "27          27       30   0.0030  0.928682  0.908821  0.948544\n",
       "62          62      100   0.0011  0.928682  0.908821  0.948544\n",
       "6            6       10   0.0011  0.930233  0.910572  0.949893\n",
       "4            4       10   0.0007  0.930233  0.910572  0.949893\n",
       "64          64      100   0.0015  0.930233  0.910572  0.949893\n",
       "34          34       50   0.0011  0.930233  0.910572  0.949893\n",
       "37          37       50   0.0016  0.930233  0.910572  0.949893\n",
       "20          20       30   0.0011  0.930233  0.910572  0.949893\n",
       "21          21       30   0.0013  0.930233  0.910572  0.949893\n",
       "10          10       10   0.0018  0.931783  0.912326  0.951240\n",
       "67          67      100   0.0020  0.931783  0.912326  0.951240\n",
       "66          66      100   0.0018  0.931783  0.912326  0.951240\n",
       "65          65      100   0.0016  0.931783  0.912326  0.951240\n",
       "61          61      100   0.0009  0.931783  0.912326  0.951240\n",
       "9            9       10   0.0016  0.931783  0.912326  0.951240\n",
       "25          25       30   0.0020  0.931783  0.912326  0.951240\n",
       "13          13       10   0.0030  0.931783  0.912326  0.951240\n",
       "12          12       10   0.0025  0.931783  0.912326  0.951240\n",
       "11          11       10   0.0020  0.931783  0.912326  0.951240\n",
       "36          36       50   0.0015  0.931783  0.912326  0.951240\n",
       "26          26       30   0.0025  0.933333  0.914083  0.952584\n",
       "63          63      100   0.0013  0.933333  0.914083  0.952584\n",
       "24          24       30   0.0018  0.933333  0.914083  0.952584\n",
       "8            8       10   0.0015  0.934884  0.915842  0.953925\n",
       "23          23       30   0.0016  0.934884  0.915842  0.953925"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
