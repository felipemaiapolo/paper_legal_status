{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maiapolo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing: opening auxiliary files and defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that cleans texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(resulta):   \n",
    "    import copy \n",
    "   \n",
    "    result = copy.deepcopy(resulta)\n",
    "    \n",
    "    result=result.lower()\n",
    "    result=\" \"+result #colocando espaço no começo\n",
    "    result=re.sub('\\d', ' ', result)\n",
    "    result=result.replace(\"lei \", \"lei_\")\n",
    "    result=result.replace(\"lei nº \", \"lei_\")\n",
    "    result=result.replace(\"lei n.º\" ,\"lei_\")\n",
    "    result=result.replace(\"lei estadual nº \", \"lei_\") \n",
    "    result=result.replace(\"lei federal nº \", \"lei_\") \n",
    "    result=result.replace(\"lei municipal nº \", \"lei_\")\n",
    "    result=result.replace(\"fl. \", \"fls. \")\n",
    "    result=result.replace(\"fls. \", \"fls_\") \n",
    "    result=result.replace(\"p. \", \"pp. \")\n",
    "    result=result.replace(\"pp. \", \"pp_\")\n",
    "    result=result.replace(\"art. \", \"art_\") \n",
    "    result=result.replace(\"artigo \", \"art_\")\n",
    "    result=result.replace(\"inciso \", \"inciso_\") \n",
    "    result=result.replace(\"nº \", \"nº_\")\n",
    "    result=result.replace(\"n° \", \"nº_\")\n",
    "    result=result.replace(\"º \", \"º\")\n",
    "    result=result.replace(\"ª \", \"ª\")\n",
    "    result=result.replace(\"oab \", \"oab_\")\n",
    "    result=result.replace(\"r$ \", \"r$_\")\n",
    "    result=result.replace(\"\\n\", \" \")\n",
    "    result=result.replace(\"dr \", \"dr_\")\n",
    "    result=result.replace(\"dr. \", \"dr_\")\n",
    "    result=result.replace(\"dra \", \"dr_\")\n",
    "    result=result.replace(\"dra. \", \"dr_\")\n",
    "    result=result.replace(\"adv: \", \"adv_\") \n",
    "    \n",
    "    result=result.replace(\"/\", \" \")\n",
    "    result=result.replace(\"|\", \" \")\n",
    "    result=result.replace(\"+\", \" \")\n",
    "    result=result.replace(\".\", \" \")\n",
    "    result=result.replace(\",\", \" \")\n",
    "    result=result.replace(\":\", \" \")\n",
    "    result=result.replace(\";\", \" \")\n",
    "    result=result.replace(\"!\", \" \")\n",
    "    result=result.replace(\"?\", \" \")\n",
    "    result=result.replace(\">\", \" \")\n",
    "    result=result.replace(\"=\", \" \")\n",
    "    result=result.replace(\"§\", \" \")\n",
    "    result=result.replace(\" - \", \" \")\n",
    "    result=result.replace(\" _ \", \" \")\n",
    "    result=result.replace(\"&\", \" \")\n",
    "    result=result.replace(\"*\", \" \")\n",
    "    result=result.replace(\"(\", \" \")\n",
    "    result=result.replace(\")\", \" \")\n",
    "    result=result.replace(\"ª\", \" \")\n",
    "    result=result.replace(\"º\", \" \")\n",
    "    result=result.replace(\"%\", \" \")\n",
    "    result=result.replace(\"[\", \" \")\n",
    "    result=result.replace(\"]\", \" \")\n",
    "    result=result.replace(\"{\", \" \")\n",
    "    result=result.replace(\"}\", \" \")\n",
    "    result=result.replace(\"'\", \" \")\n",
    "    result=result.replace('\"', \" \")\n",
    "    result=result.replace(\"“\", \" \")\n",
    "    result=result.replace(\"”\", \" \")\n",
    "    result=re.sub(' +', ' ', result)\n",
    "    result=result+\" \" #colocando espaço no fim\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening unlabeled motions dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bases/mov_treino.txt\", \"rb\") as fp:   # Unpickling\n",
    "    mov = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning texts and storing them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.38 minutos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "texts_mov=[]\n",
    "\n",
    "for i in range(len(mov)):\n",
    "    for j in range(len(mov[i])):\n",
    "        texts_mov.append(clean(unidecode.unidecode(mov[i][j][1])))\n",
    "print(round((time.time() - start_time)/60,2),\"minutos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening algorithms trained in the  Word2/Vec & Doc2Vec notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams=Phrases.load('modelos/bigrams_mov')\n",
    "bibigrams=Phrases.load('modelos/bibigrams_mov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizador de textos\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def tokenize_tfidf(txt):\n",
    "    texto=txt\n",
    "    texto=texto.split(' ') \n",
    "    texto=bibigrams[bigrams[texto]]\n",
    "    tokens=[]\n",
    "    for t in texto:\n",
    "        if t not in stop_words: tokens.append(t)\n",
    "        else: pass\n",
    " \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.167833630243937"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, max_features=2000,\n",
    "                             ngram_range=(1, 1), tokenizer=tokenize_tfidf)\n",
    "vectorizer.fit(texts_mov)\n",
    "\n",
    "(time.time() - start_time)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modelos/tfidf_mov_V1.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
